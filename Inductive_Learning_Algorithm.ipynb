{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need of ILA in presence of other machine learning algorithms:\n",
    "The ILA is a new algorithm which was needed even when other reinforcement learnings like ID3 and AQ were available.\n",
    "1. The need was due to the pitfalls which were present in the previous algorithms, one of the major pitfalls was lack of generalisation of rules\n",
    "\n",
    "2. The ID3 and AQ used the decision tree production method which was too specific which were difficult to analyse and was very slow to perform for basic short classification problems.\n",
    "\n",
    "3. The decision tree-based algorithm was unable to work for a new problem if some attributes are missing.\n",
    "\n",
    "4. The ILA uses the method of production of a general set of rules instead of decision trees, which overcome the above problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ILA ALGORITHM:\n",
    "    General requirements at start of algorithm:-\n",
    "   \n",
    "list the examples in the form of a table ‘T’ where each row corresponds to an example and each column contains an attribute value.\n",
    "\n",
    "create a set of m training examples, each example composed of k attributes and a class attribute with n possible decisions.\n",
    "\n",
    "create a rule set, R, having the initial value false.\n",
    "\n",
    "initially all rows in the table are unmarked.\n",
    "\n",
    "Step 1:\n",
    "divide the table ‘T’ containing m examples into n sub-tables (t1, t2,…..tn). One table for each possible value of the class attribute. (repeat steps 2-8 for each sub-table)\n",
    "\n",
    "Step 2:\n",
    "\n",
    "Initialize the attribute combination count ‘ j ‘ = 1.\n",
    "\n",
    "Step 3:\n",
    "For the sub-table on which work is going on, divide the attribute list into distinct combinations, each combination with ‘j ‘ distinct attributes.\n",
    "\n",
    "\n",
    "Step 4:\n",
    "\n",
    "For each combination of attributes, count the number of occurrences of attribute values that appear under the same combination of attributes in unmarked rows of the sub-table under consideration, and at the same time, not appears under the same combination of attributes of other sub-tables\n",
    "\n",
    "If ‘MAX’ = = null , increase ‘ j ‘ by 1 and go to Step 3.\n",
    "\n",
    "Mark all rows of the sub-table where working, in which the values of ‘MAX’ appear, as classi?ed.\n",
    "\n",
    "Add a rule (IF attribute = “XYZ” –> THEN decision is YES/ NO) to R whose left-hand side will have attribute names of the ‘MAX’ with their values separated by AND, and its right-hand side contains the decision attribute value associated with the sub-table\n",
    "\n",
    "If all rows are marked as classi?ed, then move on to process another sub-table and go to Step 2. else, go to Step 4. If no sub-tables are available, exit with the set of rules obtained till then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/inductive-learning-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the iris dataset as an example(as inbuilt in sckitlearn)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the feature matrix (X) and response vector (y)\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the feature and target names\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# printing features and target names of our dataset\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type of X is: <class 'numpy.ndarray'>\n",
      "\n",
      "First 5 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# X and y are numpy arrays\n",
    "print(\"\\nType of X is:\", type(X))\n",
    "# printing first 5 input rows\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading external dataset: Now, consider the case when we want to load an external dataset. For this purpose, we can use pandas library for easily loading and manipulating dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, important data types are:\n",
    "\n",
    "Series: Series is a one-dimensional labeled array capable of holding any data type.\n",
    "\n",
    "DataFrame: It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load a dataset\n",
    "A dataset is nothing but a collection of data. A dataset generally has two main components:\n",
    "\n",
    "Features: (also known as predictors, inputs, or attributes) they are simply the variables of our data. They can be more than one and hence represented by a feature matrix (‘X’ is a common notation to represent feature matrix). A list of all the feature names is termed as feature names.\n",
    "Response: (also known as the target, label, or output) This is the output variable depending on the feature variables. We generally have a single response column and it is represented by a response vector (‘y’ is a common notation to represent response vector). All the possible values taken by a response vector is termed as target names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (25, 2)\n",
      "\n",
      "Features: Index(['Hours', 'Scores'], dtype='object')\n",
      "\n",
      "Feature matrix:\n",
      "    Hours\n",
      "0    2.5\n",
      "1    5.1\n",
      "2    3.2\n",
      "3    8.5\n",
      "4    3.5\n",
      "\n",
      "Response vector:\n",
      " 0    21\n",
      "1    47\n",
      "2    27\n",
      "3    75\n",
      "4    30\n",
      "Name: Scores, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# reading csv file\n",
    "data = pd.read_csv('student_scores.csv')\n",
    "  \n",
    "# shape of dataset\n",
    "print(\"Shape:\", data.shape)\n",
    "  \n",
    "# column names\n",
    "print(\"\\nFeatures:\", data.columns)\n",
    "  \n",
    "# storing the feature matrix (X) and response vector (y)\n",
    "X = data[data.columns[:-1]]\n",
    "y = data[data.columns[-1]]\n",
    "  \n",
    "# printing first 5 rows of feature matrix\n",
    "print(\"\\nFeature matrix:\\n\", X.head())\n",
    "  \n",
    "# printing first 5 values of response vector\n",
    "print(\"\\nResponse vector:\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Splitting the dataset\n",
    "Now, in order to determine their accuracy, one can train the model using the given dataset and then predict the response values for the same dataset using that model and hence, find the accuracy of the model\n",
    "A better option is to split our data into two parts: first one for training our machine learning model, and second one for testing our model.\n",
    "To summarize:\n",
    "\n",
    "Split the dataset into two pieces: a training set and a testing set.\n",
    "Train the model on the training set.\n",
    "Test the model on the testing set, and evaluate how well our model did.\n",
    "Advantages of train/test split:\n",
    "\n",
    "Model can be trained and tested on different data than the one used for training.\n",
    "Response values are known for the test dataset, hence predictions can be evaluated\n",
    "Testing accuracy is a better estimate than training accuracy of out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(60, 4)\n",
      "(90,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset as an example\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "  \n",
    "# store the feature matrix (X) and response vector (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "  \n",
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "  \n",
    "# printing the shapes of the new X objects\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "  \n",
    "# printing the shapes of the new y objects\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split function takes several arguments which are explained below:\n",
    "X, y: These are the feature matrix and response vector which need to be splitted.\n",
    "test_size: It is the ratio of test data to the given data. For example, setting test_size = 0.4 for 150 rows of X produces test data of 150 x 0.4 = 60 rows.\n",
    "random_state: If you use random_state = some_number, then you can guarantee that your split will be always the same. This is useful if you want reproducible results, for example in testing for consistency in the documentation (so that everybody can see the same numbers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Training the model\n",
    "Scikit-learn provides a wide range of machine learning algorithms which have a unified/consistent interface for fitting, predicting accuracy, etc.\n",
    "\n",
    "The example given below uses KNN (K nearest neighbors) classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the iris dataset as an example\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "  \n",
    "# store the feature matrix (X) and response vector (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "  \n",
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "  \n",
    "# training the model on training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN model accuracy: 0.9833333333333333\n",
      "Predictions: ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# making predictions on the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "from sklearn import metrics\n",
    "print(\"kNN model accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "# making prediction for out of sample data\n",
    "sample = [[3, 5, 4, 2], [2, 3, 5, 4]]\n",
    "preds = knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in preds]\n",
    "print(\"Predictions:\", pred_species)\n",
    "  \n",
    "# saving the model\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(knn, 'iris_knn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important points to note from the above code:\n",
    "\n",
    "We create a knn classifier object using:\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "The classifier is trained using X_train data. The process is termed as fitting. We pass the feature matrix and the corresponding response vector.\n",
    "knn.fit(X_train, y_train)\n",
    "Now, we need to test our classifier on the X_test data. knn.predict method is used for this purpose. It returns the predicted response vector, y_pred.\n",
    "y_pred = knn.predict(X_test)\n",
    "Now, we are interested in finding the accuracy of our model by comparing y_test and y_pred. This is done using metrics module’s method accuracy_score:\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "Consider the case when you want your model to make prediction on out of sample data. Then, the sample input can simply pe passed in the same way as we pass any feature matrix.\n",
    "sample = [[3, 5, 4, 2], [2, 3, 5, 4]]\n",
    "preds = knn.predict(sample)\n",
    "If you are not interested in training your classifier again and again and use the pre-trained classifier, one can save their classifier using joblib. All you need to do is:\n",
    "joblib.dump(knn, 'iris_knn.pkl')\n",
    "In case you want to load an already saved classifier, use the following method:\n",
    "knn = joblib.load('iris_knn.pkl')\n",
    "As we approach the end of this article, here are some benefits of using scikit-learn over some other machine learning libraries(like R libraries):\n",
    "\n",
    "Consistent interface to machine learning models\n",
    "Provides many tuning parameters but with sensible defaults\n",
    "Exceptional documentation\n",
    "Rich set of functionality for companion tasks.\n",
    "Active community for development and support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
